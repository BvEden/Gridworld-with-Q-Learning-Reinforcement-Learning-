{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The agent-environment interaction: how to get started\n",
    "\n",
    "This notebook presents _one possible_ approach to get you started in the agent-environment interaction exercise. You are highly encouraged to make your own decisions and possibly deviate from this template to increase the learning effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General structure\n",
    "\n",
    "We want to implement the agent-environment interface depicted in the exercise sheet. We need\n",
    "* an **agent** with a random policy,\n",
    "* an **environment**,\n",
    "* and a procedure that lets the agent interact within the environment.\n",
    "\n",
    "We will build each of these components incrementally and components as they are needed. We make the choice of defining a **class** for both the agent and the environment. \n",
    "\n",
    "For the learning procedure itself, we would eventually organise the code in a double (nested) loop; the outer one for episodes, and the inner one for steps within each episode. However, we will start looking at just **one single step**. In this step, we need to\n",
    "1. observe the current state of the environment,\n",
    "2. let the agent choose an action, and\n",
    "3. let the environment respond by changing its state and returning a reward.\n",
    "\n",
    "Written differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_t = environment.get_current_state()\n",
    "# action_t = agent.choose_action(state_t)\n",
    "# reward_t, state_t_plus_1 = environment.make_step(action_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus need an agent with a `choose_action(state_t)` method.\n",
    "And we need an environment with a `get_current_state()` method and a `make_step(action_t)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment: a Gridworld class\n",
    "\n",
    "We will distringuish between the internal state of the environment which is hidden from the agent and the interface, that is, the parts that are accessible to the agent (for example, we do not want the agent to know where the bomb is, but the agent has to know which actions are available).\n",
    "\n",
    "#### The internal state of the environment\n",
    "The Gridworld class has to keep track of\n",
    "* the general properties of the environment, e.g., the available actions or the number of cells\n",
    "* the positions of the agent, and both terminal states: the gold and the bomb\n",
    "* the rewards for each cell\n",
    "\n",
    "In order to keep things simple, we will implement a very unflexible version of the environment. For example, we will keep the dimensions of the grid, the number of bombs, and the position of the gold fixed. You can (read: should) introduce variables for all these hard-coded properties once your algorithm is working.\n",
    "\n",
    "Let us first define a class that specifies all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "        \n",
    "        # Choose starting position of the agent randomly among the first 5 cells\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "        \n",
    "        # Choose position of the gold and bomb\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.terminal_states = np.array([self.bomb_positions, self.gold_positions])\n",
    "       \n",
    "        # Specify rewards\n",
    "        self.rewards = np.zeros(self.num_cells)\n",
    "        self.rewards[self.bomb_positions] = -10\n",
    "        self.rewards[self.gold_positions] = 10\n",
    "        \n",
    "        # Specify available actions\n",
    "        self.actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "    \n",
    "    ## Put methods here:\n",
    "    def get_available_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def make_step(self, action): \n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        \n",
    "        # Update new_position based on the chosen action and check whether agent hits a wall.\n",
    "        if action == \"UP\":\n",
    "            candidate_position = self.agent_position + self.num_cols\n",
    "            if candidate_position < self.num_cells:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"RIGHT\":\n",
    "            candidate_position = self.agent_position + 1\n",
    "            if candidate_position % self.num_cols > 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"DOWN\":\n",
    "            candidate_position = self.agent_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"LEFT\": \n",
    "            candidate_position = self.agent_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "        \n",
    "        # Update the position of the agent.\n",
    "        self.agent_position = new_position\n",
    "        \n",
    "        # Get reward \n",
    "        reward = self.rewards[new_position]\n",
    "        \n",
    "        # Deduct 1 from reward if agent moved\n",
    "        if old_position != new_position:\n",
    "            reward -= 1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we chose to specify the cells in a one-dimensional array instead of a two-dimensional array (e.g., the bomb is at position `18` and not at position `[3,4]` in a two-dimensional array). This is a completely personal choice and you could choose either option. \n",
    "\n",
    "#### The interface of the environment\n",
    "We now want to define a method that provides any agent with the needed (and allowed!) information about the current state of the environment.\n",
    "\n",
    "Take a minute and think about which fields among those we defined above are needed both for the RandomAgent and, later, the Q-learning agent. \n",
    "\n",
    "...? You found an answer? Go on.\n",
    "\n",
    "The RandomAgent only needs the available actions (technically, only the number of available actions) to randomly choose from. The Q-learning agent will need the available actions _and_ its current position.\n",
    "\n",
    "Let us for now focus on the random agent and define a method for the Gridworld class that returns the available actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent: a RandomAgent class\n",
    "\n",
    "We now define a RandomAgent class with a `choose_action` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def choose_action(self, available_actions):\n",
    "        number_of_actions = len(available_actions)\n",
    "        random_index = np.random.randint(0, number_of_actions)\n",
    "        action = available_actions[random_index]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code that we've written so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available_actions = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
      "Randomly chosen action = LEFT\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "agent = RandomAgent()\n",
    "\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the agent has chosen a random action, we need the environment to respond to that action. Let us define another method for the `Gridworld` class that takes the chosen action as input, updates the internal state of the environment, and returns a reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having updated the Gridworld class with the `make_step` method, create a new Gridworld..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and execute the following cell multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the agent = 2\n",
      "Available_actions = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
      "Randomly chosen action = RIGHT\n",
      "Reward obtained = -1.0\n",
      "Current position of the agent = 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Current position of the agent =\", env.agent_position)\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)\n",
    "reward = env.make_step(chosen_action)\n",
    "print(\"Reward obtained =\", reward)\n",
    "print(\"Current position of the agent =\", env.agent_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now manually walk the RandomAgent through our gridworld!\n",
    "\n",
    "Next steps include:\n",
    "* Build a loop so that you do not have to manually move the agent.\n",
    "* Define a new class that implements a Q-learning Agent\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
